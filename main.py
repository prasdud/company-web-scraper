# -----------------------------
# BOILERPLATE CODE GENERATED BY GPT5
# LOGIC WRITTEN BY ABDUL
# AUGUST 13 1:09 PM
# -----------------------------

import threading
import pandas as pd
from queue import Queue
from googlesearch import search
import requests

# -----------------------------
# Configuration / Constants
# -----------------------------
NUM_WORKERS = 5        # 1 worker per 10 companies (adjustable)
COMPANIES_FILE = "data.csv"  # Input data
MAX_TOTAL_JOBS = 200   # Stop after 200 jobs

# -----------------------------
# Shared Data Structures
# -----------------------------
company_queue = Queue()     # Companies to process
collected_jobs = []         # All scraped job postings (thread-safe access needed)
collected_jobs_lock = threading.Lock()
data = pd.read_csv(COMPANIES_FILE)

# -----------------------------
# Functions
# -----------------------------

def find_company_info(company_name, company_description):

    website_dork = f"{company_name} {company_description} site:.com"
    linkedin_dork = f"{company_name} {company_description} site:linkedin.com"
    careers_page_dork = f"{company_name} {company_description} site:.com careers"
    jobs_page_dork = f"{company_name} {company_description} site:.com jobs"

    def get_first_result(query):
        try:
            for url in search(query, num_results=1):
                return url
        except:
            return ""
        return ""

    website_url = get_first_result(website_dork)
    linkedin_url = get_first_result(linkedin_dork)
    careers_page_url = get_first_result(careers_page_dork)
    jobs_page_url = get_first_result(jobs_page_dork)



    return {
        "website": website_url,
        "linkedin": linkedin_url,
        "careers_page": careers_page_url,
        "jobs_page": jobs_page_url
    }


def scrape_jobs(jobs_page_url):
    """
    Scrape top 3 jobs from the jobs page.
    Return a list of dictionaries with job details:
        - title
        - url
        - location
        - date
        - description
    """
    # TODO: Implement scraping logic
    return []


def verify_job(job):
    """
    Verify a single job posting is valid.
    If invalid, return False.
    """
    # TODO: implement URL check, non-empty fields, etc.
    return True


def process_company(company):
    """
    Worker function: process one company
    """
    company_info = find_company_info(company)
    
    if not company_info.get("jobs_page"):
        return  # No jobs found, skip
    
    jobs = scrape_jobs(company_info["jobs_page"])
    
    for job in jobs:
        if verify_job(job):
            with collected_jobs_lock:
                if len(collected_jobs) < MAX_TOTAL_JOBS:
                    collected_jobs.append(job)
                else:
                    break


def worker():
    """
    Worker thread to process companies from the queue
    """
    while not company_queue.empty():
        company = company_queue.get()
        process_company(company)
        company_queue.task_done()

def verify_url(url):
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36"}
    try:
        if not url.startswith("http"):
            url = "https://" + url  # ensure proper scheme
        response = requests.get(url, headers=headers, timeout=5)
        return response.status_code == 200
    except:
        return False



# -----------------------------
# Main Execution
# -----------------------------
def main():
    company_name = "Willow"
    company_description = "Optimize buildings for security, sustainability, and efficiency with real-time integrated facility management."

    info = find_company_info(company_name, company_description)

    for key in info:
        url = info[key]
        if url and verify_url(url):
            print(f"{key}: {url} ✅")
        else:
            print(f"{key}: {url} ❌ (not reachable or empty)")

if __name__ == "__main__":
    main()

# -----------------------------
# BOILERPLATE CODE GENERATED BY GPT5
# LOGIC WRITTEN BY ABDUL
# AUGUST 13 1:09 PM
# -----------------------------

import threading
import pandas as pd
from queue import Queue
from googlesearch import search
import requests
from bs4 import BeautifulSoup
#import nltk
#from nltk.corpus import stopwords
#from nltk.tokenize import word_tokenize
import spacy

# -----------------------------
# Configuration / Constants
# -----------------------------
NUM_WORKERS = 5        # 1 worker per 10 companies (adjustable)
COMPANIES_FILE = "data.csv"  # Input data
MAX_TOTAL_JOBS = 200   # Stop after 200 jobs
#nltk.download('punkt')
#nltk.download('stopwords')
nlp = spacy.load("en_core_web_sm")

# -----------------------------
# Shared Data Structures
# -----------------------------
company_queue = Queue()     # Companies to process
collected_jobs = []         # All scraped job postings (thread-safe access needed)
collected_jobs_lock = threading.Lock()
data = pd.read_csv(COMPANIES_FILE)
careers_paths = ["/careers", "/jobs", "/about-us/careers", "/talent", "/join-us"]
third_party_job_sites = [
    "jobs.lever.co",
    ".zohorecruit.com/jobs/Careers",
    "boards.greenhouse.io",
    "apply.workable.com",
    "careers.smartrecruiters.com"
]
job_link_patterns = ["/job/", "/apply/", "/careers/", "/positions/", "/openings/"]
# -----------------------------
# Functions
# -----------------------------

def find_company_info(company_name, company_description):
    
    def extract_keywords(description, top_n=2):
        doc = nlp(description)
        
        # Collect noun chunks (multi-word phrases)
        phrases = [chunk.text.lower() for chunk in doc.noun_chunks]
        
        # Optionally, add single nouns that are important
        nouns = [token.text.lower() for token in doc if token.pos_ == "NOUN"]
        
        # Combine both lists
        candidates = phrases + nouns
        
        # Remove duplicates and pick top_n
        seen = set()
        keywords = []
        for word in candidates:
            if word not in seen and word.isalpha():
                keywords.append(word)
                seen.add(word)
            if len(keywords) == top_n:
                break
                
        return keywords
    
    keywords = extract_keywords(company_description)
    description_keywords = " ".join(keywords)
    print(f"Description keywords -> {description_keywords} ")

    flag_career_page_equals_jobs_page = False 

    website_dork = f'"{company_name}" {description_keywords} official website site:.com OR site:.org OR site:.ai'
    linkedin_dork = f"{company_name} {description_keywords} site:linkedin.com"
    careers_page_dork = f"{company_name} {description_keywords} (careers OR jobs) site:.com OR site:.org OR site:.ai"
    jobs_page_dork = f"{company_name} {description_keywords} jobs site:.com OR site:.org OR site:.ai"


    def get_first_result(query):
        try:
            for url in search(query, num_results=1):
                return url
        except:
            return ""
        return ""
    
    def resolve_final_url(url):
        try:
            resp = requests.head(url, allow_redirects=True, timeout=5)
            return resp.url
        except:
            return url

    
    website_url = get_first_result(website_dork)
    website_url = resolve_final_url(website_url)

    linkedin_url = get_first_result(linkedin_dork)
    # AT THIS POINT WE GET WEBSITE AND LINKEDIN

    def career_paths_check(website_url):
        for path in careers_paths:
            url_to_check = website_url.rstrip("/") + path
            resp, valid = verify_url(url_to_check, return_response=True)

            if valid:
                content_type = resp.headers.get("Content-Type", "")
                if "text/html" in content_type:
                    return url_to_check
        return ""
    
    careers_page_url = career_paths_check(website_url)
    if careers_page_url:
        careers_page_url = resolve_final_url(careers_page_url)

    def career_page_has_jobs(careers_page_url):
        resp, valid = verify_url(careers_page_url, return_response=True)

        if not valid:
            return False
        
        soup = BeautifulSoup(resp.text, "html.parser")
        page_text = soup.get_text().lower()

        for a_tag in soup.find_all("a", href=True):
            href = a_tag["href"].lower()
            for pattern in job_link_patterns:
                if pattern in href:
                    return True
                
        job_items = soup.find_all(["li", "div"], class_=lambda x: x and "job" in x.lower())
        if len(job_items) >= 1:  #threshold for confidence
            return True
        
        return False


    if career_page_has_jobs(careers_page_url):
        jobs_page_url = careers_page_url
    else:
        jobs_page_url = ""
        for portal in third_party_job_sites:
            candidate = get_first_result(f"{company_name} jobs site:{portal}") #need to check company name instead of URL
            if candidate:
                candidate = resolve_final_url(candidate)  # ensure final URL
                jobs_page_url = candidate
                break


    return {
        "website": website_url,
        "linkedin": linkedin_url,
        "careers_page": careers_page_url,
        "jobs_page": jobs_page_url
    }


def scrape_jobs(jobs_page_url):
    """
    Scrape top 3 jobs from the jobs page.
    Return a list of dictionaries with job details:
        - title
        - url
        - location
        - date
        - description
    """
    # TODO: Implement scraping logic
    return []


def verify_job(job):
    """
    Verify a single job posting is valid.
    If invalid, return False.
    """
    # TODO: implement URL check, non-empty fields, etc.
    return True


def process_company(company):
    """
    Worker function: process one company
    """
    company_info = find_company_info(company)
    
    if not company_info.get("jobs_page"):
        return  # No jobs found, skip
    
    jobs = scrape_jobs(company_info["jobs_page"])
    
    for job in jobs:
        if verify_job(job):
            with collected_jobs_lock:
                if len(collected_jobs) < MAX_TOTAL_JOBS:
                    collected_jobs.append(job)
                else:
                    break


def worker():
    """
    Worker thread to process companies from the queue
    """
    while not company_queue.empty():
        company = company_queue.get()
        process_company(company)
        company_queue.task_done()



def verify_url(url, return_response=False):
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36"}
    try:
        response = requests.get(url, headers=headers, timeout=5)
        is_valid = response.status_code == 200
        if return_response:
            return response, is_valid
        else:
            return is_valid
    except:
        if return_response:
            return None, False
        return False



# -----------------------------
# Main Execution
# -----------------------------
def main():
    # Example company
    company_name = "EcoVadis"
    company_description = "Empowering companies with actionable insights to improve sustainability and reduce environmental impact."

    # Find company info
    info = find_company_info(company_name, company_description)

    # Check and print results
    print(f"\nResults for '{company_name}':\n" + "-"*50)
    for key, url in info.items():
        if url:
            try:
                if verify_url(url):
                    status = "✅ Reachable"
                else:
                    status = "❌ Not reachable"
            except Exception as e:
                status = f"❌ Error verifying URL: {e}"
        else:
            status = "❌ Empty"
        print(f"{key}: {url} --> {status}")
    print("-"*50 + "\n")

if __name__ == "__main__":
    main()


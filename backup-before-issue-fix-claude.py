# -----------------------------
# BOILERPLATE CODE GENERATED BY GPT5
# LOGIC WRITTEN BY ABDUL
# AUGUST 13 1:09 PM
# MODIFIED TO USE GOOGLE CUSTOM SEARCH API BY CLAUDE AI
# -----------------------------

import threading
import pandas as pd
from queue import Queue
import requests
from bs4 import BeautifulSoup
import spacy
import time
from spacy.lang.en.stop_words import STOP_WORDS
import os
from dotenv import load_dotenv
from urllib.parse import urljoin
import requests
from bs4 import BeautifulSoup
import json
import re

# -----------------------------
# Configuration / Constants
# -----------------------------
NUM_WORKERS = 5        # 1 worker per 10 companies (adjustable)
COMPANIES_FILE = "data.csv"  # Input data
MAX_TOTAL_JOBS = 200   # Stop after 200 jobs
load_dotenv()

# Google Custom Search API Configuration
GOOGLE_API_KEY = os.getenv("CSE_API_KEY")
GOOGLE_CSE_ID = os.getenv("CSE_ID")
GOOGLE_API_ENDPOINT = "https://www.googleapis.com/customsearch/v1"

nlp = spacy.load("en_core_web_sm")

# -----------------------------
# Shared Data Structures
# -----------------------------
company_queue = Queue()
collected_jobs = []
collected_jobs_lock = threading.Lock()
data = pd.read_csv(COMPANIES_FILE)
careers_paths = ["/careers", "/jobs", "/about-us/careers", "/talent", "/join-us", "/career", "/en/careers", "/en-us/careers", "/company/careers", "/work-with-us", "/opportunities"]
third_party_job_sites = [
    {
        "name": "Lever",
        "patterns": [
            "https://jobs.lever.co/{company_name}",
            "https://jobs.lever.co/{company_name}/jobs"
        ]
    },
    {
        "name": "Zoho Recruit",
        "patterns": [
            "https://{company_name}.zohorecruit.com/jobs/Careers",
            "https://{company_name}.zohorecruit.com/jobs"
        ]
    },
    {
        "name": "Greenhouse",
        "patterns": [
            "https://boards.greenhouse.io/{company_name}",
        ]
    },
    {
        "name": "Workable",
        "patterns": [
            "https://{company_name}.workable.com/"
        ]
    },
    {
        "name": "SmartRecruiters",
        "patterns": [
            "https://careers.smartrecruiters.com/{company_name}",
            "https://jobs.smartrecruiters.com/{company_name}"
        ]
    }
]

job_link_patterns = ["/job/", "/apply/", "/careers/", "/positions/", "/openings/"]

# -----------------------------
# Google Custom Search API Functions
# -----------------------------

def google_search_api(query, num_results=1, site_restrict=None):
    """
    Search using Google Custom Search API
    """
    params = {
        'key': GOOGLE_API_KEY,
        'cx': GOOGLE_CSE_ID,
        'q': query,
        'num': min(num_results, 10)  # API max is 10 per request
    }
    
    if site_restrict:
        params['siteSearch'] = site_restrict
        params['siteSearchFilter'] = 'i'  # include only results from this site
    
    try:
        response = requests.get(GOOGLE_API_ENDPOINT, params=params, timeout=10)
        response.raise_for_status()
        
        data = response.json()
        
        if 'items' in data:
            return [item['link'] for item in data['items']]
        else:
            return []
            
    except requests.exceptions.RequestException as e:
        print(f"API request failed: {e}")
        return []
    except Exception as e:
        print(f"Error parsing API response: {e}")
        return []

def get_first_result_api(query, site_restrict=None):
    """
    Get first search result using Google Custom Search API
    """
    results = google_search_api(query, num_results=1, site_restrict=site_restrict)
    return results[0] if results else ""

# Rate limiting for API calls
def rate_limited_search(query, site_restrict=None, delay=0.1):
    """
    Rate-limited wrapper for API calls
    """
    result = get_first_result_api(query, site_restrict)
    time.sleep(delay)  # Small delay to respect rate limits
    return result

# -----------------------------
# Functions
# -----------------------------

def find_company_info(company_name, company_description):
    
    def extract_keywords(description, top_n=3):
        doc = nlp(description)

        keywords = []
        seen = set()

        for token in doc:
            # Keep nouns/adjectives only
            if token.pos_ in {"NOUN", "ADJ"}:
                word = token.text.lower()

                # Skip stopwords and duplicates
                if word not in STOP_WORDS and word not in seen and word.isalpha():
                    keywords.append(word)
                    seen.add(word)

        return keywords[:top_n]
    
    keywords = extract_keywords(company_description)
    description_keywords = " ".join(keywords)
    print(f"Description keywords -> {description_keywords} ")

    flag_career_page_equals_jobs_page = False 

    # Modified search queries for Google Custom Search API
    website_dork = f'"{company_name}" {description_keywords} official website'
    linkedin_dork = f"{company_name} {description_keywords}"
    careers_page_dork = f"{company_name} {description_keywords} careers OR jobs"
    jobs_page_dork = f"{company_name} {description_keywords} jobs"

    def resolve_final_url(url):
        try:
            resp = requests.head(url, allow_redirects=True, timeout=5)
            return resp.url
        except:
            return url

    # Using Google Custom Search API with site restrictions
    print(f"Searching for website...")
    website_url = rate_limited_search(website_dork)
    if website_url:
        website_url = resolve_final_url(website_url)
        print(f"Found website: {website_url}")

    print(f"Searching for LinkedIn...")
    linkedin_url = rate_limited_search(linkedin_dork, site_restrict="linkedin.com/company")
    if linkedin_url:
        print(f"Found LinkedIn: {linkedin_url}")

    def career_paths_check(website_url):
        if not website_url:
            return ""
        
        for path in careers_paths:
            url_to_check = website_url.rstrip("/") + path
            try:
                # First try HEAD
                resp = requests.head(url_to_check, timeout=10, allow_redirects=True)
                
                if resp.status_code < 400 or resp.status_code in (401, 403, 405):
                    return resp.url  # exists even if blocked
                
                # If HEAD fails badly, try GET
                resp = requests.get(url_to_check, timeout=10, allow_redirects=True)
                if resp.status_code < 400 or resp.status_code in (401, 403):
                    return resp.url
            except requests.RequestException:
                continue
        return ""


    
    print(f"Checking career paths...")
    careers_page_url = career_paths_check(website_url)
    if careers_page_url:
        careers_page_url = resolve_final_url(careers_page_url)
        print(f"Found careers page: {careers_page_url}")

    def career_page_has_jobs(careers_page_url):
        if not careers_page_url:
            return False
            
        resp, valid = verify_url(careers_page_url, return_response=True)
        if not valid:
            return False
        
        soup = BeautifulSoup(resp.text, "html.parser")
        
        # Check if page redirects to third-party job platforms
        third_party_domains = [
            "smartrecruiters.com", "workday.com", "lever.co", "greenhouse.io", 
            "workable.com", "zohorecruit.com", "bamboohr.com", "jobvite.com",
            "indeed.com", "linkedin.com/jobs"
        ]
        
        final_url = resp.url.lower()
        for domain in third_party_domains:
            if domain in final_url:
                return False  # This is actually a third-party platform, not direct jobs
        
        # Look for direct job postings on the actual careers page
        # Method 1: Individual job links that stay on same domain
        job_links = soup.find_all("a", href=True)
        direct_job_count = 0
        
        for link in job_links:
            href = link.get("href", "")
            if href.startswith('/') or careers_page_url.split('/')[2] in href:  # Same domain
                if any(pattern in href.lower() for pattern in ["/job/", "/position/", "/opening/", "/role/"]):
                    direct_job_count += 1
        
        if direct_job_count >= 1:
            return True
        
        # Method 2: Look for embedded job listings (not external redirects)
        job_containers = soup.find_all(["div", "article", "section"], 
                                    class_=lambda x: x and "job" in x.lower() if x else False)
        
        actual_jobs = 0
        for container in job_containers:
            container_text = container.get_text()
            # Must have job title AND apply mechanism AND be substantial content
            if (len(container_text) > 100 and 
                any(title in container_text.lower() for title in 
                    ["engineer", "developer", "manager", "analyst", "specialist", "coordinator"]) and
                any(action in container_text.lower() for action in ["apply", "view details", "learn more"])):
                actual_jobs += 1
        
        return actual_jobs >= 1

    def find_actual_jobs_page_from_careers(careers_page_url):
        """
        FIXED: Look for the actual jobs listing page within the careers page
        Instead of just returning the careers page URL, find the specific jobs listing URL
        """
        if not careers_page_url:
            return ""
            
        resp, valid = verify_url(careers_page_url, return_response=True)
        if not valid:
            return ""
        
        soup = BeautifulSoup(resp.text, "html.parser")
        
        # Look for links that lead to actual job listings
        job_listing_indicators = [
            "current openings", "open positions", "view jobs", "job openings", 
            "see all jobs", "browse jobs", "available positions", "all openings",
            "job opportunities", "open roles"
        ]
        
        # Method 1: Look for specific job listing page links
        all_links = soup.find_all("a", href=True)
        for link in all_links:
            href = link.get("href", "").lower()
            text = link.get_text(strip=True).lower()
            
            # Skip if it's just navigation back to careers
            if any(skip in text for skip in ["careers", "about us", "home", "contact"]):
                continue
                
            # Look for job listing page indicators in text or URL
            if any(indicator in text for indicator in job_listing_indicators):
                full_url = urljoin(careers_page_url, link.get("href"))
                # Verify this URL actually has job listings
                if verify_has_individual_job_listings(full_url):
                    return full_url
            
            # Look for URLs that indicate job listings
            if any(pattern in href for pattern in ["/jobs", "/openings", "/positions", "/opportunities"]) and href != "/careers":
                full_url = urljoin(careers_page_url, link.get("href"))
                if verify_has_individual_job_listings(full_url):
                    return full_url
        
        # Method 2: If careers page itself has individual job listings, return it
        if verify_has_individual_job_listings(careers_page_url):
            return careers_page_url
            
        return ""

    def verify_has_individual_job_listings(url):
        """
        FIXED: Verify that a page contains actual individual job postings, not just general careers info
        """
        try:
            resp, valid = verify_url(url, return_response=True)
            if not valid:
                return False
            
            soup = BeautifulSoup(resp.text, "html.parser")
            
            # Count potential individual job postings
            job_count = 0
            
            # Method 1: Look for links to individual job pages
            job_links = soup.find_all("a", href=True)
            for link in job_links:
                href = link.get("href", "").lower()
                text = link.get_text(strip=True)
                
                # Must have substantial text (likely job title) and lead to individual job page
                if (len(text) > 10 and len(text) < 100 and 
                    any(pattern in href for pattern in ["/job/", "/position/", "/opening/", "/apply/"]) and
                    not any(generic in text.lower() for generic in ["apply now", "view job", "learn more", "read more"])):
                    job_count += 1
                    
            # Method 2: Look for structured job containers
            job_containers = soup.find_all(["div", "article", "li"], 
                                         class_=lambda x: x and any(term in x.lower() for term in ["job", "position", "opening", "role"]) if x else False)
            
            for container in job_containers:
                container_text = container.get_text(strip=True)
                # Must contain job title-like content and apply mechanism
                if (len(container_text) > 50 and len(container_text) < 500 and
                    any(title_word in container_text.lower() for title_word in 
                        ["engineer", "developer", "manager", "analyst", "specialist", "coordinator", "director", "lead"]) and
                    any(apply_word in container_text.lower() for apply_word in ["apply", "view", "learn more"])):
                    job_count += 1
            
            return job_count >= 2  # Must have at least 2 job listings to be considered valid
            
        except:
            return False

    print(f"Checking if careers page has jobs...")
    jobs_page_url = ""
    
    if career_page_has_jobs(careers_page_url):
        # Issue - Instead of just using careers page, find the actual jobs listing page
        actual_jobs_page = find_actual_jobs_page_from_careers(careers_page_url)
        if actual_jobs_page:
            jobs_page_url = actual_jobs_page
            print(f"Found actual jobs listing page: {jobs_page_url}")
        else:
            jobs_page_url = careers_page_url  # Fallback to careers page
            print(f"Using careers page as jobs page: {jobs_page_url}")
    else:
        print(f"Searching for third-party job sites...")
        # Enhanced third-party site search with better targeting
        for portal in third_party_job_sites:
            portal_name = portal["name"].lower()
            candidate_url = ""
            
            for pattern in portal["patterns"]:
                
                # Method 1: Direct URL construction (most reliable)
                company_slug = company_name.lower().replace(" ", "").replace(".", "").replace("-", "")
                company_slug_dash = company_name.lower().replace(" ", "-").replace(".", "")
                
                # Try different slug variations
                #for slug in [company_slug, company_slug_dash, company_name.lower()]:
                #    test_url = pattern.format(company_name=slug)
                #    if verify_url(test_url):
                #        candidate_url = test_url
                #        print(f"Found {portal_name} via direct URL: {test_url}")
                #        break
                
                if candidate_url:
                    break
                    
                # Method 2: Targeted API search only if direct URL fails
                if "lever" in portal_name:
                    search_query = f'"{company_name}" site:jobs.lever.co'
                    results = google_search_api(search_query, num_results=3)
                    for result in results:
                        if company_name.lower().replace(" ", "") in result.lower():
                            candidate_url = result
                            break
                            
                elif "greenhouse" in portal_name:
                    search_query = f'"{company_name}" site:boards.greenhouse.io'
                    results = google_search_api(search_query, num_results=3)
                    for result in results:
                        if company_name.lower().replace(" ", "") in result.lower():
                            candidate_url = result
                            break
                            
                elif "smartrecruiters" in portal_name:
                    search_query = f'"{company_name}" site:careers.smartrecruiters.com'
                    results = google_search_api(search_query, num_results=3)
                    for result in results:
                        if company_name.lower().replace(" ", "") in result.lower():
                            candidate_url = result
                            break
                            
                
            if candidate_url:
                candidate_url = resolve_final_url(candidate_url)
                
                # Verify the portal page actually has jobs AND belongs to the right company
                if (verify_portal_has_jobs(candidate_url, portal_name) and 
                    company_name.lower().replace(" ", "") in candidate_url.lower()):
                    jobs_page_url = candidate_url
                    print(f"Found active jobs on {portal_name}: {jobs_page_url}")
                    break
                    
            # If we found jobs on this portal, stop searching other portals
            if jobs_page_url:
                break

    return {
        "website": website_url,
        "linkedin": linkedin_url,
        "careers_page": careers_page_url,
        "jobs_page": jobs_page_url
    }


def is_valid_job_title(title):
    """
    FIXED: Validate if text looks like an actual job title, not navigation text
    """
    if not title or len(title.strip()) < 3:
        return False
    
    title_lower = title.lower().strip()
    
    # Filter out obvious navigation/action words
    navigation_words = [
        "jobs", "careers", "apply now", "view job", "view all", "see all", 
        "learn more", "read more", "more details", "browse", "search",
        "open positions", "current openings", "available", "opportunities",
        "join us", "work with us", "home", "about", "contact", "back",
        "next", "previous", "filter", "sort", "location", "department",
        "full time", "part time", "remote", "hybrid", "contract",
        "submit", "upload", "resume", "cv", "apply", "application"
    ]
    
    # Reject if title is exactly a navigation word
    if title_lower in navigation_words:
        return False
    
    # Reject if title is too short and generic
    if len(title_lower) < 8 and any(nav in title_lower for nav in navigation_words):
        return False
    
    # Reject if title is too long (likely description text)
    if len(title) > 150:
        return False
    
    # Must contain at least one word that suggests it's a job title
    job_title_indicators = [
        "engineer", "developer", "manager", "director", "lead", "senior", "junior",
        "analyst", "specialist", "coordinator", "assistant", "associate",
        "representative", "consultant", "advisor", "architect", "designer",
        "scientist", "researcher", "technician", "supervisor", "executive",
        "officer", "head", "chief", "principal", "staff", "intern"
    ]
    
    # Check if it contains job title indicators
    has_job_indicator = any(indicator in title_lower for indicator in job_title_indicators)
    
    # Also accept titles that look like proper nouns (capitalized) and reasonable length
    looks_like_title = (title.count(' ') >= 1 and title.count(' ') <= 8 and 
                       any(word[0].isupper() for word in title.split() if word))
    
    return has_job_indicator or looks_like_title


def scrape_jobs(jobs_page_url, max_jobs=3):
    """
    FIXED: Scrape top 'max_jobs' from the jobs page with better job title validation
    Returns a list of dicts: {'title': ..., 'url': ...}
    """
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
                      "(KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36"
    }
    
    try:
        resp = requests.get(jobs_page_url, headers=headers, timeout=10)
        resp.raise_for_status()
    except requests.RequestException:
        return []

    soup = BeautifulSoup(resp.text, "html.parser")
    jobs = []

    # -----------------------------
    # JS-heavy portals: Look for JSON or embedded jobs
    # -----------------------------
    page_json_input = soup.find("input", {"id": "pageJson"})
    if page_json_input:
        # Zoho Recruit style JSON
        json_text = page_json_input.get("value")
        json_text = json_text.replace("&#x27;", "'").replace("&quot;", '"')
        try:
            data = json.loads(json_text)
            sections = data.get("section", {}).get("data", [])
            for block in sections:
                if len(jobs) >= max_jobs:
                    break
                if block.get("blocktype") == "jobs":
                    for job in block.get("jobs", []):
                        if len(jobs) >= max_jobs:
                            break
                        title = job.get("title") or job.get("name")
                        url = job.get("url") or job.get("link")
                        if url and not url.startswith("http"):
                            url = urljoin(jobs_page_url, url)
                        if title and url and is_valid_job_title(title):
                            jobs.append({"title": title, "url": url})
            return jobs[:max_jobs]
        except Exception as e:
            print("Error parsing JS jobs JSON:", e)
            # fallback to normal HTML scraping

    # -----------------------------
    # FIXED: HTML scraping with better job detection
    # -----------------------------
    
    # Method 1: Look for structured job containers first
    job_containers = soup.find_all(["div", "article", "li", "tr"], 
                                  class_=lambda x: x and any(term in x.lower() for term in ["job", "position", "opening", "role", "listing"]) if x else False)
    
    for container in job_containers:
        if len(jobs) >= max_jobs:
            break
            
        # Look for job title link within container
        title_link = container.find("a", href=True)
        if title_link:
            href = title_link.get("href")
            title = title_link.get_text(strip=True)
            
            if (href and title and is_valid_job_title(title) and
                any(keyword in href.lower() for keyword in ["/job", "/position", "/opening", "/apply"])):
                full_url = urljoin(jobs_page_url, href)
                jobs.append({"title": title, "url": full_url})
                continue
        
        # Alternative: look for title in heading tags within container
        for heading_tag in ["h1", "h2", "h3", "h4", "h5"]:
            heading = container.find(heading_tag)
            if heading:
                title = heading.get_text(strip=True)
                # Look for associated link
                link = heading.find("a", href=True) or container.find("a", href=True)
                if link and is_valid_job_title(title):
                    href = link.get("href")
                    if any(keyword in href.lower() for keyword in ["/job", "/position", "/opening", "/apply"]):
                        full_url = urljoin(jobs_page_url, href)
                        jobs.append({"title": title, "url": full_url})
                        break
    
    # Method 2: If no structured containers found, fall back to link scraping with strict filtering
    if not jobs:
        job_links = soup.find_all("a", href=True)
        for link in job_links:
            if len(jobs) >= max_jobs:
                break
                
            href = link.get("href")
            title = link.get_text(strip=True)
            
            if not href or not title:
                continue
            
            # Must have job-related URL pattern AND valid job title
            if (any(keyword in href.lower() for keyword in ["/job/", "/jobs/", "/position/", "/opening/"]) and
                not any(skip in href.lower() for skip in ["/jobs", "/careers", "/positions"]) and  # Skip general pages
                is_valid_job_title(title) and
                len(title.split()) >= 2):  # Job titles usually have multiple words
                
                full_url = urljoin(jobs_page_url, href)
                jobs.append({"title": title, "url": full_url})

    # Method 3: Look for JSON-LD structured data (common on modern sites)
    if not jobs:
        json_scripts = soup.find_all("script", type="application/ld+json")
        for script in json_scripts:
            try:
                json_data = json.loads(script.string)
                if isinstance(json_data, list):
                    json_data = json_data[0] if json_data else {}
                
                if json_data.get("@type") == "JobPosting":
                    title = json_data.get("title")
                    url = json_data.get("url") or jobs_page_url
                    if title and is_valid_job_title(title):
                        jobs.append({"title": title, "url": url})
                        if len(jobs) >= max_jobs:
                            break
                            
            except (json.JSONDecodeError, KeyError):
                continue

    return jobs[:max_jobs]


def verify_job(job):
    """
    Verify a single job posting is valid.
    If invalid, return False.
    """
    if not job or not isinstance(job, dict):
        return False
    
    title = job.get('title', '').strip()
    url = job.get('url', '').strip()
    
    # Must have both title and URL
    if not title or not url:
        return False
    
    # Title must pass validation
    if not is_valid_job_title(title):
        return False
    
    # URL must be valid format
    if not (url.startswith('http://') or url.startswith('https://')):
        return False
    
    return True


def process_companies_csv(file_path="data.csv", max_jobs=200):
    """
    Reads a CSV with Company Name and Company Description.
    Finds company info, scrapes up to 3 jobs per company,
    updates the CSV, and stops when max_jobs is reached.
    """
    import pandas as pd
    from tqdm import tqdm

    # Load CSV
    data = pd.read_csv(file_path)

    # Initialize counter
    total_jobs_counter = 0

    # Prepare output columns if they don't exist
    for col in [
        "Website URL", "Linkedin URL", "Careers Page URL", "Job listings page URL",
        "job post1 URL", "job post1 title",
        "job post2 URL", "job post2 title",
        "job post3 URL", "job post3 title"
    ]:
        if col not in data.columns:
            data[col] = ""

    # Loop over companies
    
    #below for all 200
    #for idx, row in tqdm(data.iterrows(), total=len(data)):
    #TEST_LIMIT = 10  # only process 5 companies for testing
    #for idx, row in tqdm(data.head(TEST_LIMIT).iterrows(), total=TEST_LIMIT):
    for idx, row in tqdm(data.iterrows(), total=len(data)):
        if total_jobs_counter >= max_jobs:
            print(f"Reached {max_jobs} jobs. Stopping.")
            break

        company_name = row["Company Name"]
        company_description = row["Company Description"]

        # -----------------------------
        # Use your existing logic
        # -----------------------------
        info = find_company_info(company_name, company_description)

        # Fill URLs in the dataframe
        data.at[idx, "Website URL"] = info.get("website", "")
        data.at[idx, "Linkedin URL"] = info.get("linkedin", "")
        data.at[idx, "Careers Page URL"] = info.get("careers_page", "")
        data.at[idx, "Job listings page URL"] = info.get("jobs_page", "")

        # -----------------------------
        # FIXED: Only scrape from proper job listing pages, not careers pages
        # -----------------------------
        jobs_page_url = info.get("jobs_page")  # Removed fallback to careers_page
        if jobs_page_url:
            jobs = scrape_jobs(jobs_page_url)
            
            # FIXED: Validate each job before adding
            valid_jobs = [job for job in jobs if verify_job(job)]
            
            for i, job in enumerate(valid_jobs[:3]):  # max 3 jobs per company
                data.at[idx, f"job post{i+1} URL"] = job["url"]
                data.at[idx, f"job post{i+1} title"] = job["title"]
                total_jobs_counter += 1

                if total_jobs_counter >= max_jobs:
                    break

    # -----------------------------
    # Save CSV
    # -----------------------------
    data.to_csv(file_path, index=False)
    print(f"Done! Total jobs collected: {total_jobs_counter}")

def verify_portal_has_jobs(portal_url, portal_name):
    if not portal_url:
        return False
        
    resp, valid = verify_url(portal_url, return_response=True)
    if not valid or not resp:
        return False
        
    soup = BeautifulSoup(resp.text, "html.parser")
    page_text = soup.get_text(separator=" ", strip=True).lower()

    # Explicit rejection phrases
    rejection_phrases = [
        "no jobs available", "no job openings", "no current openings",
        "no vacancies", "no open positions", "0 jobs found",
        "page not found", "page not available", "nothing to display", "does not exist", "no job openings at the moment"
    ]
    if any(phrase in page_text for phrase in rejection_phrases):
        return False
        
    # Portal-specific job detection logic
    if "lever" in portal_name:
        # Lever specific indicators
        job_elements = soup.find_all(["div", "a"], class_=lambda x: x and 
                                   any(term in x.lower() for term in ["posting", "job"]))
        job_links = soup.find_all("a", href=lambda x: x and "/jobs/" in x)
        return len(job_elements) >= 1 or len(job_links) >= 1
        
    elif "greenhouse" in portal_name:
        # Greenhouse specific indicators
        job_elements = soup.find_all(["div", "section"], class_=lambda x: x and "opening" in x.lower())
        job_links = soup.find_all("a", href=lambda x: x and "/jobs/" in x)
        return len(job_elements) >= 1 or len(job_links) >= 1 or "view all jobs" in page_text
        
    elif "workable" in portal_name:
        # Workable specific indicators
        job_elements = soup.find_all(["li", "div"], class_=lambda x: x and "job" in x.lower())
        return len(job_elements) >= 1 or "we're hiring" in page_text
        
    elif "zoho" in portal_name:
        # Zoho Recruit specific indicators
        job_elements = soup.find_all(["tr", "div"], class_=lambda x: x and 
                                   any(term in x.lower() for term in ["job", "vacancy", "opening"]))
        return len(job_elements) >= 1 or "apply now" in page_text
 
    elif "smartrecruiters" in portal_name:
        # SmartRecruiters specific indicators
        job_elements = soup.find_all(["div", "li"], class_=lambda x: x and "job" in x.lower())
        return len(job_elements) >= 1 or "view job" in page_text
    # Generic fallback for unknown portals
    job_indicators = [
        "apply now", "view job", "open position", "we're hiring", 
        "job opening", "current openings", "join our team"
    ]
    return any(indicator in page_text for indicator in job_indicators)


def verify_url(url, return_response=False):
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
                      "(KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36"
    }

    try:
        # Let requests follow redirects (needed for /careers that move to a different domain)
        resp = requests.get(
            url,
            headers=headers,
            timeout=5,
            allow_redirects=True,
        )

        # Treat 2xx and 3xx as valid
        valid = resp.status_code < 400

        if return_response:
            return resp, valid
        return valid

    except requests.RequestException:
        if return_response:
            return None, False
        return False



# -----------------------------
# Main Execution
# -----------------------------
def main():
    # Check API configuration
    if GOOGLE_API_KEY == "your_api_key_here" or GOOGLE_CSE_ID == "your_cse_id_here":
        print("WARNING: Please configure your Google Custom Search API credentials!")
        print("Set GOOGLE_API_KEY and GOOGLE_CSE_ID variables at the top of the file.")
        print("Get your API key from: https://developers.google.com/custom-search/v1/overview")
        print("Set up Custom Search Engine at: https://cse.google.com/")
        print("\nProceeding with example anyway...\n")
    
    # Example company
    company_name = "Fuse Energy"
    company_description = "Accelerating global renewable energy transition with comprehensive solutions."

    print(f"Processing company: {company_name}")
    print(f"Description: {company_description}\n")

    # Find company info using Google Custom Search API
    info = find_company_info(company_name, company_description)

    # Check and print company URLs
    print(f"\nResults for '{company_name}':\n" + "-"*50)
    for key, url in info.items():
        if url:
            try:
                if verify_url(url):
                    status = "Reachable"
                else:
                    status = "Not reachable"
            except Exception as e:
                status = f"Error verifying URL: {e}"
        else:
            status = "Empty"
        print(f"{key.replace('_', ' ').title()}: {url}")
        print(f"Status: {status}\n")
    print("-"*50 + "\n")

    # -----------------------------
    # Scrape latest 3 jobs if jobs_page exists
    # -----------------------------
    jobs_page_url = info.get("jobs_page") or info.get("careers_page")
    if jobs_page_url:
        print(f"Scraping jobs from: {jobs_page_url}\n")
        jobs = scrape_jobs(jobs_page_url)
        if jobs:
            for idx, job in enumerate(jobs, 1):
                print(f"Job {idx}:")
                print(f"Title: {job['title']}")
                print(f"URL: {job['url']}\n")
        else:
            print("No jobs found on this page.")
    else:
        print("No careers or jobs page found to scrape.")

    print("-"*50 + "\n")
    print("✅ Test complete.")

if __name__ == "__main__":
    process_companies_csv("data.csv", max_jobs=200)


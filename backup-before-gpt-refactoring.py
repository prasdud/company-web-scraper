# -----------------------------
# BOILERPLATE CODE GENERATED BY GPT5
# LOGIC WRITTEN BY ABDUL
# AUGUST 13 1:09 PM
# MODIFIED TO USE GOOGLE CUSTOM SEARCH API BY CLAUDE AI
# -----------------------------

import threading
import pandas as pd
from queue import Queue
import requests
from bs4 import BeautifulSoup
import spacy
import time
from spacy.lang.en.stop_words import STOP_WORDS
import os
from dotenv import load_dotenv

# -----------------------------
# Configuration / Constants
# -----------------------------
NUM_WORKERS = 5        # 1 worker per 10 companies (adjustable)
COMPANIES_FILE = "data.csv"  # Input data
MAX_TOTAL_JOBS = 200   # Stop after 200 jobs
load_dotenv

# Google Custom Search API Configuration
GOOGLE_API_KEY = os.getenv("CSE_API_KEY")  # Replace with your actual API key
GOOGLE_CSE_ID = os.getenv("CSE_ID")   # Replace with your Custom Search Engine ID
GOOGLE_API_ENDPOINT = "https://www.googleapis.com/customsearch/v1"

nlp = spacy.load("en_core_web_sm")

# -----------------------------
# Shared Data Structures
# -----------------------------
company_queue = Queue()     # Companies to process
collected_jobs = []         # All scraped job postings (thread-safe access needed)
collected_jobs_lock = threading.Lock()
data = pd.read_csv(COMPANIES_FILE)
careers_paths = ["/careers", "/jobs", "/about-us/careers", "/talent", "/join-us", "/career", "/en/careers", "/en-us/careers", "/company/careers", "/work-with-us", "/opportunities"]
third_party_job_sites = [
    {
        "name": "Lever",
        "patterns": [
            "https://jobs.lever.co/{company_name}",
            "https://jobs.lever.co/{company_name}/jobs"
        ]
    },
    {
        "name": "Zoho Recruit",
        "patterns": [
            "https://{company_name}.zohorecruit.com/jobs/Careers",
            "https://{company_name}.zohorecruit.com/jobs"
        ]
    },
    {
        "name": "Greenhouse",
        "patterns": [
            "https://boards.greenhouse.io/{company_name}",
        ]
    },
    {
        "name": "Workable",
        "patterns": [
            "https://{company_name}.workable.com/"
        ]
    },
    {
        "name": "SmartRecruiters",
        "patterns": [
            "https://careers.smartrecruiters.com/{company_name}",
            "https://jobs.smartrecruiters.com/{company_name}"
        ]
    }
]

job_link_patterns = ["/job/", "/apply/", "/careers/", "/positions/", "/openings/"]

# -----------------------------
# Google Custom Search API Functions
# -----------------------------

def google_search_api(query, num_results=1, site_restrict=None):
    """
    Search using Google Custom Search API
    """
    params = {
        'key': GOOGLE_API_KEY,
        'cx': GOOGLE_CSE_ID,
        'q': query,
        'num': min(num_results, 10)  # API max is 10 per request
    }
    
    if site_restrict:
        params['siteSearch'] = site_restrict
        params['siteSearchFilter'] = 'i'  # include only results from this site
    
    try:
        response = requests.get(GOOGLE_API_ENDPOINT, params=params, timeout=10)
        response.raise_for_status()
        
        data = response.json()
        
        if 'items' in data:
            return [item['link'] for item in data['items']]
        else:
            return []
            
    except requests.exceptions.RequestException as e:
        print(f"API request failed: {e}")
        return []
    except Exception as e:
        print(f"Error parsing API response: {e}")
        return []

def get_first_result_api(query, site_restrict=None):
    """
    Get first search result using Google Custom Search API
    """
    results = google_search_api(query, num_results=1, site_restrict=site_restrict)
    return results[0] if results else ""

# Rate limiting for API calls
def rate_limited_search(query, site_restrict=None, delay=0.1):
    """
    Rate-limited wrapper for API calls
    """
    result = get_first_result_api(query, site_restrict)
    time.sleep(delay)  # Small delay to respect rate limits
    return result

# -----------------------------
# Functions
# -----------------------------

def find_company_info(company_name, company_description):
    
    def extract_keywords(description, top_n=3):
        doc = nlp(description)

        keywords = []
        seen = set()

        for token in doc:
            # Keep nouns/adjectives only
            if token.pos_ in {"NOUN", "ADJ"}:
                word = token.text.lower()

                # Skip stopwords and duplicates
                if word not in STOP_WORDS and word not in seen and word.isalpha():
                    keywords.append(word)
                    seen.add(word)

        return keywords[:top_n]
    
    keywords = extract_keywords(company_description)
    description_keywords = " ".join(keywords)
    print(f"Description keywords -> {description_keywords} ")

    flag_career_page_equals_jobs_page = False 

    # Modified search queries for Google Custom Search API
    website_dork = f'"{company_name}" {description_keywords} official website'
    linkedin_dork = f"{company_name} {description_keywords}"
    careers_page_dork = f"{company_name} {description_keywords} careers OR jobs"
    jobs_page_dork = f"{company_name} {description_keywords} jobs"

    def resolve_final_url(url):
        try:
            resp = requests.head(url, allow_redirects=True, timeout=5)
            return resp.url
        except:
            return url

    # Using Google Custom Search API with site restrictions
    print(f"Searching for website...")
    website_url = rate_limited_search(website_dork)
    if website_url:
        website_url = resolve_final_url(website_url)
        print(f"Found website: {website_url}")

    print(f"Searching for LinkedIn...")
    linkedin_url = rate_limited_search(linkedin_dork, site_restrict="linkedin.com/company")
    if linkedin_url:
        print(f"Found LinkedIn: {linkedin_url}")

    def career_paths_check(website_url):
        if not website_url:
            return ""
        
        for path in careers_paths:
            url_to_check = website_url.rstrip("/") + path
            try:
                # First try HEAD
                resp = requests.head(url_to_check, timeout=10, allow_redirects=True)
                
                if resp.status_code < 400 or resp.status_code in (401, 403, 405):
                    return resp.url  # exists even if blocked
                
                # If HEAD fails badly, try GET
                resp = requests.get(url_to_check, timeout=10, allow_redirects=True)
                if resp.status_code < 400 or resp.status_code in (401, 403):
                    return resp.url
            except requests.RequestException:
                continue
        return ""


    
    print(f"Checking career paths...")
    careers_page_url = career_paths_check(website_url)
    if careers_page_url:
        careers_page_url = resolve_final_url(careers_page_url)
        print(f"Found careers page: {careers_page_url}")

    def career_page_has_jobs(careers_page_url):
        if not careers_page_url:
            return False
            
        resp, valid = verify_url(careers_page_url, return_response=True)
        if not valid:
            return False
        
        soup = BeautifulSoup(resp.text, "html.parser")
        
        # Check if page redirects to third-party job platforms
        third_party_domains = [
            "smartrecruiters.com", "workday.com", "lever.co", "greenhouse.io", 
            "workable.com", "zohorecruit.com", "bamboohr.com", "jobvite.com",
            "indeed.com", "linkedin.com/jobs"
        ]
        
        final_url = resp.url.lower()
        for domain in third_party_domains:
            if domain in final_url:
                return False  # This is actually a third-party platform, not direct jobs
        
        # Look for direct job postings on the actual careers page
        # Method 1: Individual job links that stay on same domain
        job_links = soup.find_all("a", href=True)
        direct_job_count = 0
        
        for link in job_links:
            href = link.get("href", "")
            if href.startswith('/') or careers_page_url.split('/')[2] in href:  # Same domain
                if any(pattern in href.lower() for pattern in ["/job/", "/position/", "/opening/", "/role/"]):
                    direct_job_count += 1
        
        if direct_job_count >= 1:
            return True
        
        # Method 2: Look for embedded job listings (not external redirects)
        job_containers = soup.find_all(["div", "article", "section"], 
                                    class_=lambda x: x and "job" in x.lower() if x else False)
        
        actual_jobs = 0
        for container in job_containers:
            container_text = container.get_text()
            # Must have job title AND apply mechanism AND be substantial content
            if (len(container_text) > 100 and 
                any(title in container_text.lower() for title in 
                    ["engineer", "developer", "manager", "analyst", "specialist", "coordinator"]) and
                any(action in container_text.lower() for action in ["apply", "view details", "learn more"])):
                actual_jobs += 1
        
        return actual_jobs >= 1

    print(f"Checking if careers page has jobs...")
    jobs_page_url = ""
    
    if career_page_has_jobs(careers_page_url):
        jobs_page_url = careers_page_url
        print(f"Careers page has jobs: {jobs_page_url}")
    else:
        print(f"Searching for third-party job sites...")
        # Enhanced third-party site search with better targeting
        for portal in third_party_job_sites:
            portal_name = portal["name"].lower()
            candidate_url = ""
            
            # Try each pattern for the current portal
            for pattern in portal["patterns"]:
                
                # Method 1: Direct URL construction (most reliable)
                company_slug = company_name.lower().replace(" ", "").replace(".", "").replace("-", "")
                company_slug_dash = company_name.lower().replace(" ", "-").replace(".", "")
                
                # Try different slug variations
                #for slug in [company_slug, company_slug_dash, company_name.lower()]:
                #    test_url = pattern.format(company_name=slug)
                #    if verify_url(test_url):
                #        candidate_url = test_url
                #        print(f"Found {portal_name} via direct URL: {test_url}")
                #        break
                
                if candidate_url:
                    break
                    
                # Method 2: Targeted API search only if direct URL fails
                if "lever" in portal_name:
                    search_query = f'"{company_name}" site:jobs.lever.co'
                    results = google_search_api(search_query, num_results=3)
                    for result in results:
                        if company_name.lower().replace(" ", "") in result.lower():
                            candidate_url = result
                            break
                            
                elif "greenhouse" in portal_name:
                    search_query = f'"{company_name}" site:boards.greenhouse.io'
                    results = google_search_api(search_query, num_results=3)
                    for result in results:
                        if company_name.lower().replace(" ", "") in result.lower():
                            candidate_url = result
                            break
                            
                elif "smartrecruiters" in portal_name:
                    search_query = f'"{company_name}" site:careers.smartrecruiters.com'
                    results = google_search_api(search_query, num_results=3)
                    for result in results:
                        if company_name.lower().replace(" ", "") in result.lower():
                            candidate_url = result
                            break
                            
                # Add similar blocks for other portals...
                
            # If we found a candidate URL, verify it has jobs
            if candidate_url:
                candidate_url = resolve_final_url(candidate_url)
                
                # Verify the portal page actually has jobs AND belongs to the right company
                if (verify_portal_has_jobs(candidate_url, portal_name) and 
                    company_name.lower().replace(" ", "") in candidate_url.lower()):
                    jobs_page_url = candidate_url
                    print(f"Found active jobs on {portal_name}: {jobs_page_url}")
                    break
                    
            # If we found jobs on this portal, stop searching other portals
            if jobs_page_url:
                break

    return {
        "website": website_url,
        "linkedin": linkedin_url,
        "careers_page": careers_page_url,
        "jobs_page": jobs_page_url
    }


def scrape_jobs(jobs_page_url):
    """
    Scrape top 3 jobs from the jobs page.
    Return a list of dictionaries with job details:
        - title
        - url
        - location
        - date
        - description
    """
    # TODO: Implement scraping logic
    return []


def verify_job(job):
    """
    Verify a single job posting is valid.
    If invalid, return False.
    """
    # TODO: implement URL check, non-empty fields, etc.
    return True


def process_company(company):
    """
    Worker function: process one company
    """
    company_info = find_company_info(company)
    
    if not company_info.get("jobs_page"):
        return  # No jobs found, skip
    
    jobs = scrape_jobs(company_info["jobs_page"])
    
    for job in jobs:
        if verify_job(job):
            with collected_jobs_lock:
                if len(collected_jobs) < MAX_TOTAL_JOBS:
                    collected_jobs.append(job)
                else:
                    break


def worker():
    """
    Worker thread to process companies from the queue
    """
    while not company_queue.empty():
        company = company_queue.get()
        process_company(company)
        company_queue.task_done()


def verify_portal_has_jobs(portal_url, portal_name):
    if not portal_url:
        return False
        
    resp, valid = verify_url(portal_url, return_response=True)
    if not valid or not resp:
        return False
        
    soup = BeautifulSoup(resp.text, "html.parser")
    page_text = soup.get_text(separator=" ", strip=True).lower()

    # Explicit rejection phrases
    rejection_phrases = [
        "no jobs available", "no job openings", "no current openings",
        "no vacancies", "no open positions", "0 jobs found",
        "page not found", "page not available", "nothing to display", "does not exist", "no job openings at the moment"
    ]
    if any(phrase in page_text for phrase in rejection_phrases):
        return False
        
    # Portal-specific job detection logic
    if "lever" in portal_name:
        # Lever specific indicators
        job_elements = soup.find_all(["div", "a"], class_=lambda x: x and 
                                   any(term in x.lower() for term in ["posting", "job"]))
        job_links = soup.find_all("a", href=lambda x: x and "/jobs/" in x)
        return len(job_elements) >= 1 or len(job_links) >= 1
        
    elif "greenhouse" in portal_name:
        # Greenhouse specific indicators
        job_elements = soup.find_all(["div", "section"], class_=lambda x: x and "opening" in x.lower())
        job_links = soup.find_all("a", href=lambda x: x and "/jobs/" in x)
        return len(job_elements) >= 1 or len(job_links) >= 1 or "view all jobs" in page_text
        
    elif "workable" in portal_name:
        # Workable specific indicators
        job_elements = soup.find_all(["li", "div"], class_=lambda x: x and "job" in x.lower())
        return len(job_elements) >= 1 or "we're hiring" in page_text
        
    elif "zoho" in portal_name:
        # Zoho Recruit specific indicators
        job_elements = soup.find_all(["tr", "div"], class_=lambda x: x and 
                                   any(term in x.lower() for term in ["job", "vacancy", "opening"]))
        return len(job_elements) >= 1 or "apply now" in page_text
 
    elif "smartrecruiters" in portal_name:
        # SmartRecruiters specific indicators
        job_elements = soup.find_all(["div", "li"], class_=lambda x: x and "job" in x.lower())
        return len(job_elements) >= 1 or "view job" in page_text
    # Generic fallback for unknown portals
    job_indicators = [
        "apply now", "view job", "open position", "we're hiring", 
        "job opening", "current openings", "join our team"
    ]
    return any(indicator in page_text for indicator in job_indicators)


def verify_url(url, return_response=False):
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
                      "(KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36"
    }

    try:
        # Let requests follow redirects (needed for /careers that move to a different domain)
        resp = requests.get(
            url,
            headers=headers,
            timeout=5,
            allow_redirects=True,
        )

        # Treat 2xx and 3xx as valid
        valid = resp.status_code < 400

        if return_response:
            return resp, valid
        return valid

    except requests.RequestException:
        if return_response:
            return None, False
        return False


# -----------------------------
# Main Execution
# -----------------------------
def main():
    # Check API configuration
    if GOOGLE_API_KEY == "your_api_key_here" or GOOGLE_CSE_ID == "your_cse_id_here":
        print("⚠️  WARNING: Please configure your Google Custom Search API credentials!")
        print("Set GOOGLE_API_KEY and GOOGLE_CSE_ID variables at the top of the file.")
        print("Get your API key from: https://developers.google.com/custom-search/v1/overview")
        print("Set up Custom Search Engine at: https://cse.google.com/")
        print("\nProceeding with example anyway...\n")
    
    # Example company
    company_name = "Plus"
    company_description = "AI-powered autonomous driving for a safer, greener world."

    print(f"Processing company: {company_name}")
    print(f"Description: {company_description}\n")

    # Find company info using Google Custom Search API
    info = find_company_info(company_name, company_description)

    # Check and print results
    print(f"\nResults for '{company_name}':\n" + "-"*50)
    for key, url in info.items():
        if url:
            try:
                if verify_url(url):
                    status = "✅ Reachable"
                else:
                    status = "❌ Not reachable"
            except Exception as e:
                status = f"❌ Error verifying URL: {e}"
        else:
            status = "❌ Empty"
        print(f"{key.replace('_', ' ').title()}: {url}")
        print(f"Status: {status}\n")
    print("-"*50 + "\n")

if __name__ == "__main__":
    main()
# -----------------------------
# BOILERPLATE CODE GENERATED BY GPT5
# LOGIC WRITTEN BY ABDUL
# AUGUST 13 1:09 PM
# -----------------------------

import threading
import pandas as pd
from queue import Queue
from googlesearch import search
import requests
from bs4 import BeautifulSoup
import spacy

# -----------------------------
# Configuration / Constants
# -----------------------------
NUM_WORKERS = 5        # 1 worker per 10 companies
COMPANIES_FILE = "data.csv"
MAX_TOTAL_JOBS = 200
nlp = spacy.load("en_core_web_sm")

# -----------------------------
# Shared Data Structures
# -----------------------------
company_queue = Queue()     # Companies to process
collected_jobs = []         # All scraped job postings (thread-safe access needed)
collected_jobs_lock = threading.Lock()
data = pd.read_csv(COMPANIES_FILE)
careers_paths = ["/careers", "/jobs", "/about-us/careers", "/talent", "/join-us"]
third_party_job_sites = [
    {
        "name": "Lever",
        "patterns": [
            "https://jobs.lever.co/{company_name}",
            "https://jobs.lever.co/{company_name}/jobs"
        ]
    },
    {
        "name": "Zoho Recruit",
        "patterns": [
            "https://{company_name}.zohorecruit.com/jobs/Careers",
            "https://{company_name}.zohorecruit.com/jobs"
        ]
    },
    {
        "name": "Greenhouse",
        "patterns": [
            "https://boards.greenhouse.io/{company_name}",
        ]
    },
    {
        "name": "Workable",
        "patterns": [
            "https://{company_name}.workable.com/"
        ]
    },
    {
        "name": "Indeed / Climatebase",
        "patterns": [
            "https://climatebase.org/company/{company_name}/jobs"
        ]
    }
]

job_link_patterns = ["/job/", "/apply/", "/careers/", "/positions/", "/openings/"]
# -----------------------------
# Functions
# -----------------------------

def find_company_info(company_name, company_description):
    
    def extract_keywords(description, top_n=2):
        doc = nlp(description)
        
        # Collect noun chunks (multi-word phrases)
        phrases = [chunk.text.lower() for chunk in doc.noun_chunks]
        
        # Optionally, add single nouns that are important
        nouns = [token.text.lower() for token in doc if token.pos_ == "NOUN"]
        
        # Combine both lists
        candidates = phrases + nouns
        
        # Remove duplicates and pick top_n
        seen = set()
        keywords = []
        for word in candidates:
            if word not in seen and word.isalpha():
                keywords.append(word)
                seen.add(word)
            if len(keywords) == top_n:
                break
                
        return keywords
    
    keywords = extract_keywords(company_description)
    description_keywords = " ".join(keywords)
    print(f"Description keywords -> {description_keywords} ")

    flag_career_page_equals_jobs_page = False 

    website_dork = f'"{company_name}" {description_keywords} official website site:.com OR site:.org OR site:.ai'
    linkedin_dork = f"{company_name} {description_keywords} site:linkedin.com"
    careers_page_dork = f"{company_name} {description_keywords} (careers OR jobs) site:.com OR site:.org OR site:.ai"
    jobs_page_dork = f"{company_name} {description_keywords} jobs site:.com OR site:.org OR site:.ai"


    def get_first_result(query):
        try:
            for url in search(query, num_results=1):
                return url
        except:
            return ""
        return ""
    
    def resolve_final_url(url):
        try:
            resp = requests.head(url, allow_redirects=True, timeout=5)
            return resp.url
        except:
            return url

    
    website_url = get_first_result(website_dork)
    website_url = resolve_final_url(website_url)

    linkedin_url = get_first_result(linkedin_dork)
    # AT THIS POINT WE GET WEBSITE AND LINKEDIN

    def career_paths_check(website_url):
        if not website_url:
            return ""
        
        for path in careers_paths:
            url_to_check = website_url.rstrip("/") + path
            try:
                resp = requests.get(url_to_check, timeout=10, allow_redirects=True)
                if resp.status_code == 200:
                    content_type = resp.headers.get("Content-Type", "")
                    if "text/html" in content_type:
                        return resp.url  # return final URL after redirect
            except requests.RequestException:
                continue
        return ""
    
    careers_page_url = career_paths_check(website_url)
    if careers_page_url:
        careers_page_url = resolve_final_url(careers_page_url)

    def career_page_has_jobs(careers_page_url):
        resp, valid = verify_url(careers_page_url, return_response=True)

        if not valid:
            return False
        
        soup = BeautifulSoup(resp.text, "html.parser")
        page_text = soup.get_text().lower()

        for a_tag in soup.find_all("a", href=True):
            href = a_tag["href"].lower()
            for pattern in job_link_patterns:
                if pattern in href:
                    return True
                
        job_items = soup.find_all(["li", "div"], class_=lambda x: x and "job" in x.lower())
        if len(job_items) >= 1:  #threshold for confidence
            return True
        
        return False


    if career_page_has_jobs(careers_page_url):
        jobs_page_url = careers_page_url
    else:
        jobs_page_url = ""
        for portal in third_party_job_sites:
            candidate = get_first_result(f"{company_name} jobs site:{portal}") #need to check company name instead of URL
            if candidate:
                candidate = resolve_final_url(candidate)  # ensure final URL
                jobs_page_url = candidate
                break


    return {
        "website": website_url,
        "linkedin": linkedin_url,
        "careers_page": careers_page_url,
        "jobs_page": jobs_page_url
    }


def scrape_jobs(jobs_page_url):
    """
    Scrape top 3 jobs from the jobs page.
    Return a list of dictionaries with job details:
        - title
        - url
        - location
        - date
        - description
    """
    # TODO: Implement scraping logic
    return []


def verify_job(job):
    """
    Verify a single job posting is valid.
    If invalid, return False.
    """
    # TODO: implement URL check, non-empty fields, etc.
    return True


def process_company(company):
    """
    Worker function: process one company
    """
    company_info = find_company_info(company)
    
    if not company_info.get("jobs_page"):
        return  # No jobs found, skip
    
    jobs = scrape_jobs(company_info["jobs_page"])
    
    for job in jobs:
        if verify_job(job):
            with collected_jobs_lock:
                if len(collected_jobs) < MAX_TOTAL_JOBS:
                    collected_jobs.append(job)
                else:
                    break


def worker():
    """
    Worker thread to process companies from the queue
    """
    while not company_queue.empty():
        company = company_queue.get()
        process_company(company)
        company_queue.task_done()



def verify_url(url, return_response=False):
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36"}
    try:
        response = requests.get(url, headers=headers, timeout=5)
        is_valid = response.status_code == 200
        if return_response:
            return response, is_valid
        else:
            return is_valid
    except:
        if return_response:
            return None, False
        return False



# -----------------------------
# Main Execution
# -----------------------------
def main():
    # Example company
    company_name = "EcoVadis"
    company_description = "Empowering companies with actionable insights to improve sustainability and reduce environmental impact."

    # Find company info
    info = find_company_info(company_name, company_description)

    # Check and print results
    print(f"\nResults for '{company_name}':\n" + "-"*50)
    for key, url in info.items():
        if url:
            try:
                if verify_url(url):
                    status = "✅ Reachable"
                else:
                    status = "❌ Not reachable"
            except Exception as e:
                status = f"❌ Error verifying URL: {e}"
        else:
            status = "❌ Empty"
        print(f"{key}: {url} --> {status}")
    print("-"*50 + "\n")

if __name__ == "__main__":
    main()


# -----------------------------
# BOILERPLATE CODE GENERATED BY GPT5
# LOGIC WRITTEN BY ABDUL
# AUGUST 13 1:09 PM
# MODIFIED TO USE GOOGLE CUSTOM SEARCH API
# -----------------------------

import threading
import pandas as pd
from queue import Queue
import requests
from bs4 import BeautifulSoup
import spacy
import time

# -----------------------------
# Configuration / Constants
# -----------------------------
NUM_WORKERS = 5        # 1 worker per 10 companies (adjustable)
COMPANIES_FILE = "data.csv"  # Input data
MAX_TOTAL_JOBS = 200   # Stop after 200 jobs

# Google Custom Search API Configuration
GOOGLE_API_KEY = ""  # Replace with your actual API key
GOOGLE_CSE_ID = ""   # Replace with your Custom Search Engine ID
GOOGLE_API_ENDPOINT = "https://www.googleapis.com/customsearch/v1"

nlp = spacy.load("en_core_web_sm")

# -----------------------------
# Shared Data Structures
# -----------------------------
company_queue = Queue()     # Companies to process
collected_jobs = []         # All scraped job postings (thread-safe access needed)
collected_jobs_lock = threading.Lock()
data = pd.read_csv(COMPANIES_FILE)
careers_paths = ["/careers", "/jobs", "/about-us/careers", "/talent", "/join-us"]
third_party_job_sites = [
    {
        "name": "Lever",
        "patterns": [
            "https://jobs.lever.co/{company_name}",
            "https://jobs.lever.co/{company_name}/jobs"
        ]
    },
    {
        "name": "Zoho Recruit",
        "patterns": [
            "https://{company_name}.zohorecruit.com/jobs/Careers",
            "https://{company_name}.zohorecruit.com/jobs"
        ]
    },
    {
        "name": "Greenhouse",
        "patterns": [
            "https://boards.greenhouse.io/{company_name}",
        ]
    },
    {
        "name": "Workable",
        "patterns": [
            "https://{company_name}.workable.com/"
        ]
    },
    {
        "name": "Indeed / Climatebase",
        "patterns": [
            "https://climatebase.org/company/{company_name}/jobs"
        ]
    }
]

job_link_patterns = ["/job/", "/apply/", "/careers/", "/positions/", "/openings/"]

# -----------------------------
# Google Custom Search API Functions
# -----------------------------

def google_search_api(query, num_results=1, site_restrict=None):
    """
    Search using Google Custom Search API
    """
    params = {
        'key': GOOGLE_API_KEY,
        'cx': GOOGLE_CSE_ID,
        'q': query,
        'num': min(num_results, 10)  # API max is 10 per request
    }
    
    if site_restrict:
        params['siteSearch'] = site_restrict
        params['siteSearchFilter'] = 'i'  # include only results from this site
    
    try:
        response = requests.get(GOOGLE_API_ENDPOINT, params=params, timeout=10)
        response.raise_for_status()
        
        data = response.json()
        
        if 'items' in data:
            return [item['link'] for item in data['items']]
        else:
            return []
            
    except requests.exceptions.RequestException as e:
        print(f"API request failed: {e}")
        return []
    except Exception as e:
        print(f"Error parsing API response: {e}")
        return []

def get_first_result_api(query, site_restrict=None):
    """
    Get first search result using Google Custom Search API
    """
    results = google_search_api(query, num_results=1, site_restrict=site_restrict)
    return results[0] if results else ""

# Rate limiting for API calls
def rate_limited_search(query, site_restrict=None, delay=0.1):
    """
    Rate-limited wrapper for API calls
    """
    result = get_first_result_api(query, site_restrict)
    time.sleep(delay)  # Small delay to respect rate limits
    return result

# -----------------------------
# Functions (keeping all your existing functions)
# -----------------------------

def find_company_info(company_name, company_description):
    
    def extract_keywords(description, top_n=2):
        doc = nlp(description)
        
        # Collect noun chunks (multi-word phrases)
        phrases = [chunk.text.lower() for chunk in doc.noun_chunks]
        
        # Optionally, add single nouns that are important
        nouns = [token.text.lower() for token in doc if token.pos_ == "NOUN"]
        
        # Combine both lists
        candidates = phrases + nouns
        
        # Remove duplicates and pick top_n
        seen = set()
        keywords = []
        for word in candidates:
            if word not in seen and word.isalpha():
                keywords.append(word)
                seen.add(word)
            if len(keywords) == top_n:
                break
                
        return keywords
    
    keywords = extract_keywords(company_description)
    description_keywords = " ".join(keywords)
    print(f"Description keywords -> {description_keywords} ")

    flag_career_page_equals_jobs_page = False 

    # Modified search queries for Google Custom Search API
    website_dork = f'"{company_name}" {description_keywords} official website'
    linkedin_dork = f"{company_name} {description_keywords}"
    careers_page_dork = f"{company_name} {description_keywords} careers OR jobs"
    jobs_page_dork = f"{company_name} {description_keywords} jobs"

    def resolve_final_url(url):
        try:
            resp = requests.head(url, allow_redirects=True, timeout=5)
            return resp.url
        except:
            return url

    # Using Google Custom Search API with site restrictions
    print(f"Searching for website...")
    website_url = rate_limited_search(website_dork)
    if website_url:
        website_url = resolve_final_url(website_url)
        print(f"Found website: {website_url}")

    print(f"Searching for LinkedIn...")
    linkedin_url = rate_limited_search(linkedin_dork, site_restrict="linkedin.com")
    if linkedin_url:
        print(f"Found LinkedIn: {linkedin_url}")

    def career_paths_check(website_url):
        if not website_url:
            return ""
        
        for path in careers_paths:
            url_to_check = website_url.rstrip("/") + path
            resp, valid = verify_url(url_to_check, return_response=True)

            if valid:
                content_type = resp.headers.get("Content-Type", "")
                if "text/html" in content_type:
                    return url_to_check
        return ""
    
    print(f"Checking career paths...")
    careers_page_url = career_paths_check(website_url)
    if careers_page_url:
        careers_page_url = resolve_final_url(careers_page_url)
        print(f"Found careers page: {careers_page_url}")

    def career_page_has_jobs(careers_page_url):
        if not careers_page_url:
            return False
            
        resp, valid = verify_url(careers_page_url, return_response=True)

        if not valid:
            return False
        
        soup = BeautifulSoup(resp.text, "html.parser")
        page_text = soup.get_text().lower()

        for a_tag in soup.find_all("a", href=True):
            href = a_tag["href"].lower()
            for pattern in job_link_patterns:
                if pattern in href:
                    return True
                
        job_items = soup.find_all(["li", "div"], class_=lambda x: x and "job" in x.lower())
        if len(job_items) >= 1:  #threshold for confidence
            return True
        
        return False

    print(f"Checking if careers page has jobs...")
    jobs_page_url = ""
    
    if career_page_has_jobs(careers_page_url):
        jobs_page_url = careers_page_url
        print(f"Careers page has jobs: {jobs_page_url}")
    else:
        print(f"Searching for third-party job sites...")
        # Fixed third-party site search logic using API
        for portal in third_party_job_sites:
            portal_name = portal["name"].lower()
            
            # Search for company on specific job platforms
            if "lever" in portal_name:
                search_query = f"{company_name} jobs"
                candidate = rate_limited_search(search_query, site_restrict="jobs.lever.co")
            elif "greenhouse" in portal_name:
                search_query = f"{company_name} jobs"
                candidate = rate_limited_search(search_query, site_restrict="boards.greenhouse.io")
            elif "workable" in portal_name:
                search_query = f"{company_name} jobs"
                candidate = rate_limited_search(search_query, site_restrict="workable.com")
            elif "zoho" in portal_name:
                search_query = f"{company_name} jobs"
                candidate = rate_limited_search(search_query, site_restrict="zohorecruit.com")
            elif "climatebase" in portal_name:
                search_query = f"{company_name} jobs"
                candidate = rate_limited_search(search_query, site_restrict="climatebase.org")
            else:
                continue
                
            if candidate:
                candidate = resolve_final_url(candidate)
                jobs_page_url = candidate
                print(f"Found jobs on {portal_name}: {jobs_page_url}")
                break

    return {
        "website": website_url,
        "linkedin": linkedin_url,
        "careers_page": careers_page_url,
        "jobs_page": jobs_page_url
    }


def scrape_jobs(jobs_page_url):
    """
    Scrape top 3 jobs from the jobs page.
    Return a list of dictionaries with job details:
        - title
        - url
        - location
        - date
        - description
    """
    # TODO: Implement scraping logic
    return []


def verify_job(job):
    """
    Verify a single job posting is valid.
    If invalid, return False.
    """
    # TODO: implement URL check, non-empty fields, etc.
    return True


def process_company(company):
    """
    Worker function: process one company
    """
    company_info = find_company_info(company)
    
    if not company_info.get("jobs_page"):
        return  # No jobs found, skip
    
    jobs = scrape_jobs(company_info["jobs_page"])
    
    for job in jobs:
        if verify_job(job):
            with collected_jobs_lock:
                if len(collected_jobs) < MAX_TOTAL_JOBS:
                    collected_jobs.append(job)
                else:
                    break


def worker():
    """
    Worker thread to process companies from the queue
    """
    while not company_queue.empty():
        company = company_queue.get()
        process_company(company)
        company_queue.task_done()


def verify_url(url, return_response=False):
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36"}
    try:
        response = requests.get(url, headers=headers, timeout=5)
        is_valid = response.status_code == 200
        if return_response:
            return response, is_valid
        else:
            return is_valid
    except:
        if return_response:
            return None, False
        return False


# -----------------------------
# Main Execution
# -----------------------------
def main():
    # Check API configuration
    if GOOGLE_API_KEY == "your_api_key_here" or GOOGLE_CSE_ID == "your_cse_id_here":
        print("⚠️  WARNING: Please configure your Google Custom Search API credentials!")
        print("Set GOOGLE_API_KEY and GOOGLE_CSE_ID variables at the top of the file.")
        print("Get your API key from: https://developers.google.com/custom-search/v1/overview")
        print("Set up Custom Search Engine at: https://cse.google.com/")
        print("\nProceeding with example anyway...\n")
    
    # Example company
    company_name = "EcoVadis"
    company_description = "Empowering companies with actionable insights to improve sustainability and reduce environmental impact."

    print(f"Processing company: {company_name}")
    print(f"Description: {company_description}\n")

    # Find company info using Google Custom Search API
    info = find_company_info(company_name, company_description)

    # Check and print results
    print(f"\nResults for '{company_name}':\n" + "-"*50)
    for key, url in info.items():
        if url:
            try:
                if verify_url(url):
                    status = "✅ Reachable"
                else:
                    status = "❌ Not reachable"
            except Exception as e:
                status = f"❌ Error verifying URL: {e}"
        else:
            status = "❌ Empty"
        print(f"{key.replace('_', ' ').title()}: {url}")
        print(f"Status: {status}\n")
    print("-"*50 + "\n")

if __name__ == "__main__":
    main()